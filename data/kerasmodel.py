from keras.utils import to_categorical
from keras.models import Sequential
from keras.layers import Dense, Flatten, Dropout, Conv3D, MaxPooling3D, BatchNormalization
from keras import optimizers, backend
import h5py
import numpy as np
from time import time
from keras.callbacks import EarlyStopping, ReduceLROnPlateau
import os
import seaborn as sns
import matplotlib.pyplot as plt
import pickle
import gc
import tensorflow as tf

#enable the garbage collector
gc.enable()

#setting current directories and paths since conda envs interact weird
#specifically add graphviz to path for use with generating model pngs
os.environ["PATH"] += os.pathsep + 'C:\\Program Files (x86)\\Graphviz2.38\\bin'
#set working directory
os.chdir('C:/Users/rajes/OneDrive/Documents/ELEC498renew/ELEC-498-Capstone/data')

#import data from cleaned data file (generated by dataRead.py)
h5f = h5py.File('cleandata.h5', 'r')
b = h5f['dataset_x'][:]
c = h5f['dataset_y'][:]



#convert labels into a one-hot encoded format
g = to_categorical(c)

#uncomment the following if you want to see data plotted before each time you run

labels = [0,0,0]
for things in g:
    if np.array_equal(things,[1,0,0]):
        labels[0] += 1
    elif np.array_equal(things,[0,1,0]):
        labels[1] += 1
    elif np.array_equal(things,[0,0,1]):
        labels[2] += 1
#plot categorized data
sns.barplot(x = [0,1,2], y = labels)
plt.xlabel("Class Labels", fontsize = 20)
plt.ylabel("Number of Samples", fontsize = 20)
plt.title("Data Distributions", fontsize = 25)
plt.show()


#shuffle the data in unison to get a random test and training set
rng_state = np.random.get_state()
np.random.shuffle(b)
np.random.set_state(rng_state)
np.random.shuffle(g)


#determine indeces of the training data limits
train_amount = int((len(b)*4)/5)

#split training and testing images
train_x, test_x = b[:train_amount,:], b[train_amount:, :]
#split training and testing labels
train_y, test_y = g[:train_amount,:], g[train_amount:, :]



##############################################################################################################################
#the following is for testing a singular model, change the parameters below to determine the characteristics of the model

#number of convolution layers in each run
convlayers = [2]
denselayers =[3]
layersizes = [2048]
kernelsizes = [3]
numepochs = 40
dropoutrates = [0.1]
activationfuncs = ['elu']
learningrates = [0.01]
#whether or not to simply save final model in an h5 file
simplesavemodel = True
#whehter or not to stop early if stagnated
earlystop = False
#whether or not to reduce learning rate after plateauing
plateau = True
#whether or not to save history of the training in a pickle file
savehistory = True


#end of parameterization
##############################################################################################################################
##############################################################################################################################
#begin iterating over different sets of parameters, creating new sets of callbacks, new name, and new keras model each time, and saving each attempt into files
for convlayer in convlayers:
    for denselayer in denselayers:
        for layersize in layersizes:
            for kernelsize in kernelsizes:
                for dropoutrate in dropoutrates:
                    for learningrate in learningrates:
                        for activationfunc in activationfuncs:
                            #construct name (format is value and then name of value)
                            NAME = "{}-conv-{}-nodes-{}-dense-{}-kernelsize-{}-activation-{}-dropoutrate-{}-learningrate-timestamp-{}".format(convlayer, layersize, denselayer, kernelsize, activationfunc, dropoutrate, learningrate, int(time()))
                            print(NAME)
                            #the list of callbacks to be used when fitting the model
                            cbs = []
                            if earlystop:
                                es = EarlyStopping(monitor = 'val_acc', min_delta = '0.05', patience = '10', verbose = 0, mode = 'max', baseline = None)
                                #add to list of callbacks
                                cbs.append(es)
                            if plateau:
                                rp = ReduceLROnPlateau(monitor = 'val_acc', factor = 0.1, patience = 5, verbose = 0, mode = 'max', min_delta = 0.05, cooldown = 2, min_lr = 0.0000001)
                                #add to list of callbacks
                                cbs.append(rp)

                            #begin model construction
                            model = Sequential()
                            #add convolution and pooling layers
                            for i in range(convlayer):
                                model.add(Conv3D(filters = 32, kernel_size = (1, kernelsize, kernelsize), strides = (1, 1, 1), activation  = 'relu', bias_initializer = 'glorot_uniform', input_shape = (24,31,31,3)))
                                model.add(MaxPooling3D(pool_size = (2,2,2)))
                            #add flatten layer
                            model.add(Flatten())
                            #add dense layers
                            #bias = initializers.RandomNormal(mean = 0.5, stddev = 0.05, seed = None)
                            for i in range(denselayer):
                                model.add(Dense(units = layersize, activation = activationfunc, bias_initializer = 'glorot_uniform'))
                                model.add(BatchNormalization())
                                model.add(Dropout(dropoutrate))
                            #add output layer
                            model.add(Dense(units = 3, activation = 'softmax', bias_initializer = 'glorot_uniform'))
                            #compile model
                            #create SGD optimizer
                            sgd = optimizers.SGD(lr = learningrate, clipnorm = 1)
                            model.compile(optimizer = sgd, loss = 'categorical_crossentropy', metrics = ['accuracy'])


                            #fit model with appropriate callbacks
                            history = model.fit(train_x, train_y, validation_data = (test_x, test_y), epochs = numepochs, callbacks=cbs)



                            os.chdir('F:/ELEC498modelsdense/')
                            os.mkdir('F:/ELEC498modelsdense/' + NAME)
                            os.chdir('F:/ELEC498modelsdense/' + NAME)
                            #save the model
                            if simplesavemodel:
                                model.save(NAME + ".h5")
                            #save the training history endcoded as pickle file
                            if savehistory:
                                with open(NAME + '.pkl', 'wb') as histfile:
                                    pickle.dump(history.history, histfile)

                            #frees up GPU memory so that multiple models can be run in succession
                            del model
                            del history
                            gc.collect()
                            backend.clear_session()
                            tf.reset_default_graph()

#end of iteration                                
##############################################################################################################################


                    



